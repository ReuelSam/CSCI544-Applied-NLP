{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reuel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\reuel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reuel\\AppData\\Local\\Temp\\ipykernel_19364\\47645999.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv(\"dataset/amazon_reviews_us_Beauty_v1_00.tsv\", sep=\"\\t\", on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"dataset/amazon_reviews_us_Beauty_v1_00.tsv\", sep=\"\\t\", on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe.loc[:, ['star_rating', 'review_body', 'review_headline']]\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "df['review_headline'] = df['review_headline'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignClass(rating):\n",
    "    if rating == 1 or rating == 2:\n",
    "        return 1\n",
    "    elif rating == 3:\n",
    "        return 2\n",
    "    elif rating == 4 or rating == 5:\n",
    "        return 3\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['star_rating'].map(assignClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[df[\"class\"] == 1].sample(n=20000)\n",
    "df_2 = df[df[\"class\"] == 2].sample(n=20000)\n",
    "df_3 = df[df[\"class\"] == 3].sample(n=20000)\n",
    "# print(len(df_1), len(df_2), len(df_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2, df_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe that is being worked on:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4550602</th>\n",
       "      <td>1</td>\n",
       "      <td>I am so upset and wanted to return. However th...</td>\n",
       "      <td>picture shows 17 brushes.the titel says 13 pc ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368402</th>\n",
       "      <td>2</td>\n",
       "      <td>horrid name!  kids like but  why the name?</td>\n",
       "      <td>kids like but why the name</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184565</th>\n",
       "      <td>2</td>\n",
       "      <td>Its very thick and I could have gotten it chea...</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878729</th>\n",
       "      <td>1</td>\n",
       "      <td>Do not order this product it is not the origin...</td>\n",
       "      <td>FAKE!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013017</th>\n",
       "      <td>1</td>\n",
       "      <td>Ive waxed before so im not new to painfully ri...</td>\n",
       "      <td>DONT BUY THIS!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635167</th>\n",
       "      <td>1</td>\n",
       "      <td>The scarf does not look anything like the pict...</td>\n",
       "      <td>DOES NOT LOOK LIKE THE PICTURE!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572617</th>\n",
       "      <td>1</td>\n",
       "      <td>A friend bought some of this sunscreen for me,...</td>\n",
       "      <td>Very bad experience</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530113</th>\n",
       "      <td>2</td>\n",
       "      <td>I developed a fondness for the June Jacobs Gre...</td>\n",
       "      <td>Underwhelmed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966424</th>\n",
       "      <td>2</td>\n",
       "      <td>This stuff works, but be careful! it actually ...</td>\n",
       "      <td>burned my gum</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183200</th>\n",
       "      <td>2</td>\n",
       "      <td>Doesn't heat up enough.  I had it on my hair f...</td>\n",
       "      <td>Leaves much to be desired</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  \\\n",
       "4550602           1  I am so upset and wanted to return. However th...   \n",
       "2368402           2         horrid name!  kids like but  why the name?   \n",
       "2184565           2  Its very thick and I could have gotten it chea...   \n",
       "3878729           1  Do not order this product it is not the origin...   \n",
       "3013017           1  Ive waxed before so im not new to painfully ri...   \n",
       "3635167           1  The scarf does not look anything like the pict...   \n",
       "3572617           1  A friend bought some of this sunscreen for me,...   \n",
       "3530113           2  I developed a fondness for the June Jacobs Gre...   \n",
       "4966424           2  This stuff works, but be careful! it actually ...   \n",
       "4183200           2  Doesn't heat up enough.  I had it on my hair f...   \n",
       "\n",
       "                                           review_headline  class  \n",
       "4550602  picture shows 17 brushes.the titel says 13 pc ...      1  \n",
       "2368402                         kids like but why the name      1  \n",
       "2184565                                          Two Stars      1  \n",
       "3878729                                            FAKE!!!      1  \n",
       "3013017                                    DONT BUY THIS!!      1  \n",
       "3635167                    DOES NOT LOOK LIKE THE PICTURE!      1  \n",
       "3572617                                Very bad experience      1  \n",
       "3530113                                       Underwhelmed      1  \n",
       "4966424                                      burned my gum      1  \n",
       "4183200                          Leaves much to be desired      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataframe that is being worked on:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate length of raw reviews before any cleaning or preprocessing\n",
    "len_before_cleaning = (df['review_headline'] + \". \" + df['review_body']).apply(len).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove urls                                                                                                             \n",
    "    text = re.sub(r'(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)',' ',text)\n",
    "    # remove email ids    \n",
    "    text = re.sub(r'([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)',' ',text)       \n",
    "    # html tag                                             \n",
    "    text = re.sub('<[^<]+?>', '', text)                                                                                            \n",
    "    \n",
    "    # expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # replace non aplha numeric characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]',' ',text)                                                                                      \n",
    "    # remove isolated characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # remove consecutively repeating words                                                                                     \n",
    "    text = re.sub(r'\\b(\\w+)(?:\\W+\\1\\b)+', r'\\1', text, flags=re.IGNORECASE)\n",
    "    # replace every word following not/never/no as NEG_word until a fullstop is found\n",
    "    text = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s]+[^\\w\\s]',lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)), text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)                                                                                                \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_body = df[\"review_body\"].apply(clean_data)\n",
    "review_headline = df[\"review_headline\"].apply(clean_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing length of raw uncleaned review with length of cleaned review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313.3815, 337.23428333333334\n"
     ]
    }
   ],
   "source": [
    "len_after_cleaning = (review_headline + \". \" + review_body).apply(len).mean()\n",
    "print(f\"{len_before_cleaning}, {len_after_cleaning}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the cleaned data that will NOT be pre-processed\n",
    "reviews_vanilla = review_headline.str.upper() + \". \" + review_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\reuel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('not')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = [w for w in text.split() if not w in stop_words]\n",
    "\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_body = review_body.apply(remove_stop_words)\n",
    "review_headline = review_headline.apply(remove_stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\reuel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\reuel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(reviews):\n",
    "    lemmatized_reviews = []\n",
    "    for sent in reviews.values:\n",
    "        tagged = pos_tag(sent.split(\" \"))\n",
    "        temp_review = \"\"\n",
    "        for token in tagged:\n",
    "            tag = get_wordnet_pos(token[1])\n",
    "            if tag:\n",
    "                temp_review += lemmatizer.lemmatize(token[0], tag)\n",
    "            else:\n",
    "                temp_review += token[0]\n",
    "            temp_review += \" \"\n",
    "\n",
    "        lemmatized_reviews.append(temp_review)\n",
    "\n",
    "    return  pd.Series(lemmatized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_body = lemmatization(review_body)\n",
    "review_headline = lemmatization(review_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the cleaned and pre-processed data\n",
    "reviews_processed = review_headline.str.upper() + \". \" + review_body"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing length of review that is NOT preprocessed with length of review before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337.23428333333334, 337.23428333333334\n"
     ]
    }
   ],
   "source": [
    "len_without_preprocessing = reviews_vanilla.apply(len).mean()\n",
    "\n",
    "print(f\"{len_after_cleaning}, {len_without_preprocessing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of cleaned Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4550602    PICTURE SHOWS 17 TITEL SAYS 13 PC AND RECEIVED...\n",
       "2368402    KIDS LIKE BUT WHY THE NAME. horrid name kids l...\n",
       "2184565    TWO STARS. its very thick and could have gotte...\n",
       "3878729    FAKE . do not NEG_order NEG_this NEG_product N...\n",
       "3013017    DO NOT BUY THIS . i have waxed before so am no...\n",
       "3635167    DOES NOT LOOK LIKE THE PICTURE . the scarf doe...\n",
       "3572617    VERY BAD EXPERIENCE. a friend bought some of t...\n",
       "3530113    UNDERWHELMED. i developed fondness for the jun...\n",
       "4966424    BURNED MY GUM. this stuff works but be careful...\n",
       "4183200    LEAVES MUCH TO BE DESIRED. does not NEG_heat N...\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample of cleaned Data:\")\n",
    "reviews_vanilla.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing length of review that is preprocessed with length of review before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337.23428333333334, 249.59886666666668\n"
     ]
    }
   ],
   "source": [
    "len_after_preprocessing = reviews_processed.apply(len).mean()\n",
    "\n",
    "print(f\"{len_after_cleaning}, {len_after_preprocessing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of preprocessed Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    PICTURE SHOW 17 TITEL SAY 13 PC RECEIVE 12 . u...\n",
       "1           KID LIKE NAME . horrid name kid like name \n",
       "2    TWO STAR . thick could get cheap local beauty ...\n",
       "3    FAKE . not NEG_order NEG_this NEG_product NEG_...\n",
       "4    NOT BUY . waxed not NEG_new NEG_to NEG_painful...\n",
       "5    NOT LOOK LIKE PICTURE . scarf not look anythin...\n",
       "6    BAD EXPERIENCE . friend buy sunscreen know yea...\n",
       "7    UNDERWHELMED . develop fondness june jacob gre...\n",
       "8    BURN GUM . stuff work careful actually burn gu...\n",
       "9    LEAF MUCH DESIRE . not NEG_heat NEG_up NEG_eno...\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample of preprocessed Data:\")\n",
    "reviews_processed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_vanilla, X_test_vanilla, Y_train_vanilla, Y_test_vanilla = train_test_split(reviews_vanilla, df['class'], test_size = 0.2, random_state = 0, stratify=df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed, X_test_processed, Y_train_processed, Y_test_processed = train_test_split(reviews_pr  ocessed, df['class'], test_size = 0.2, random_state = 0, stratify=df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vect_vanilla = TfidfVectorizer(use_idf=True, max_features=5000)\n",
    "X_train_tfidf_vanilla = tf_idf_vect_vanilla.fit_transform(X_train_vanilla)\n",
    "X_test_tfidf_vanilla = tf_idf_vect_vanilla.transform(X_test_vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cleaned/vanilla data:\n",
      "\tX_train shape: (48000, 5000)\n",
      "\tY_train shape: (48000,)\n",
      "\n",
      "\tX_test shape: (12000, 5000)\n",
      "\tY_test shape: (12000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For cleaned/vanilla data:\")\n",
    "print(f\"\\tX_train shape: {X_train_tfidf_vanilla.shape}\")\n",
    "print(f\"\\tY_train shape: {Y_train_vanilla.shape}\\n\")\n",
    "\n",
    "print(f\"\\tX_test shape: {X_test_tfidf_vanilla.shape}\")\n",
    "print(f\"\\tY_test shape: {Y_test_vanilla.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect_processed = TfidfVectorizer(use_idf=True, max_features=5000)\n",
    "X_train_tfidf_processed = tf_idf_vect_processed.fit_transform(X_train_processed)\n",
    "X_test_tfidf_processed = tf_idf_vect_processed.transform(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pre-processed data:\n",
      "\tX_train shape: (48000, 5000)\n",
      "\tY_train shape: (48000,)\n",
      "\n",
      "\tX_test shape: (12000, 5000)\n",
      "\tY_test shape: (12000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For pre-processed data:\")\n",
    "print(f\"\\tX_train shape: {X_train_tfidf_processed.shape}\")\n",
    "print(f\"\\tY_train shape: {Y_train_processed.shape}\\n\")\n",
    "\n",
    "print(f\"\\tX_test shape: {X_test_tfidf_processed.shape}\")\n",
    "print(f\"\\tY_test shape: {Y_test_processed.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Stop Word Removal and Without Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999166666666667\n",
      "0.7331666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "prctrn_vanilla = Perceptron(tol=1e-3, random_state=0, validation_fraction=0.3)\n",
    "prctrn_vanilla.fit(X_train_tfidf_vanilla, Y_train_vanilla)\n",
    "\n",
    "# training accuracy\n",
    "print(prctrn_vanilla.score(X_train_tfidf_vanilla, Y_train_vanilla))\n",
    "\n",
    "# testing accuracy\n",
    "print(prctrn_vanilla.score(X_test_tfidf_vanilla, Y_test_vanilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.73      0.73      4000\n",
      "           2       0.67      0.63      0.65      4000\n",
      "           3       0.78      0.84      0.81      4000\n",
      "\n",
      "    accuracy                           0.73     12000\n",
      "   macro avg       0.73      0.73      0.73     12000\n",
      "weighted avg       0.73      0.73      0.73     12000\n",
      "\n",
      "0.7402696514881709, 0.7275, 0.7338292775185978\n",
      "0.6719083155650319, 0.63025, 0.6504127966976265\n",
      "0.7799397729905027, 0.84175, 0.8096669472165444\n",
      "0.7307059133479018, 0.7331666666666666, 0.7313030071442562\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "prctrn_pred_vanilla = prctrn_vanilla.predict(X_test_tfidf_vanilla)\n",
    "print(classification_report(Y_test_vanilla, prctrn_pred_vanilla))\n",
    "\n",
    "# class-wise precision, recall and f1-score\n",
    "prctrn_report_vanilla = classification_report(Y_test_vanilla, prctrn_pred_vanilla, output_dict=True)\n",
    "print(f\"{prctrn_report_vanilla['1']['precision']}, {prctrn_report_vanilla['1']['recall']}, {prctrn_report_vanilla['1']['f1-score']}\")\n",
    "print(f\"{prctrn_report_vanilla['2']['precision']}, {prctrn_report_vanilla['2']['recall']}, {prctrn_report_vanilla['2']['f1-score']}\")\n",
    "print(f\"{prctrn_report_vanilla['3']['precision']}, {prctrn_report_vanilla['3']['recall']}, {prctrn_report_vanilla['3']['f1-score']}\")\n",
    "print(f\"{prctrn_report_vanilla['macro avg']['precision']}, {prctrn_report_vanilla['macro avg']['recall']}, {prctrn_report_vanilla['macro avg']['f1-score']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stop Word Removal and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7800416666666666\n",
      "0.7121666666666666\n"
     ]
    }
   ],
   "source": [
    "prctrn_processed = Perceptron(tol=1e-3, random_state=0, validation_fraction=0.3)\n",
    "prctrn_processed.fit(X_train_tfidf_processed, Y_train_processed)\n",
    "\n",
    "# training accuracy\n",
    "print(prctrn_processed.score(X_train_tfidf_processed, Y_train_processed))\n",
    "\n",
    "# testing accuracy\n",
    "print(prctrn_processed.score(X_test_tfidf_processed, Y_test_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.77      0.72      4000\n",
      "           2       0.66      0.57      0.61      4000\n",
      "           3       0.79      0.79      0.79      4000\n",
      "\n",
      "    accuracy                           0.71     12000\n",
      "   macro avg       0.71      0.71      0.71     12000\n",
      "weighted avg       0.71      0.71      0.71     12000\n",
      "\n",
      "0.6790177592633194, 0.77425, 0.7235136082233383\n",
      "0.6633663366336634, 0.5695, 0.6128598331988162\n",
      "0.7917602996254681, 0.79275, 0.7922548407245471\n",
      "0.7113814651741502, 0.7121666666666666, 0.7095427607155672\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "prctrn_pred_processed = prctrn_processed.predict(X_test_tfidf_processed)\n",
    "print(classification_report(Y_test_processed, prctrn_pred_processed))\n",
    "\n",
    "# class-wise precision, recall and f1-score\n",
    "prctrn_report_processed = classification_report(Y_test_processed, prctrn_pred_processed, output_dict=True)\n",
    "print(f\"{prctrn_report_processed['1']['precision']}, {prctrn_report_processed['1']['recall']}, {prctrn_report_processed['1']['f1-score']}\")\n",
    "print(f\"{prctrn_report_processed['2']['precision']}, {prctrn_report_processed['2']['recall']}, {prctrn_report_processed['2']['f1-score']}\")\n",
    "print(f\"{prctrn_report_processed['3']['precision']}, {prctrn_report_processed['3']['recall']}, {prctrn_report_processed['3']['f1-score']}\")\n",
    "print(f\"{prctrn_report_processed['macro avg']['precision']}, {prctrn_report_processed['macro avg']['recall']}, {prctrn_report_processed['macro avg']['f1-score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Stop Word Removal and Without Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843125\n",
      "0.77325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "SVM_vanilla = LinearSVC()\n",
    "SVM_vanilla.fit(X_train_tfidf_vanilla, Y_train_vanilla)\n",
    "\n",
    "# training accuracy\n",
    "print(SVM_vanilla.score(X_train_tfidf_vanilla, Y_train_vanilla))\n",
    "\n",
    "# test accuracy\n",
    "print(SVM_vanilla.score(X_test_tfidf_vanilla, Y_test_vanilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.78      0.78      4000\n",
      "           2       0.70      0.68      0.69      4000\n",
      "           3       0.84      0.86      0.85      4000\n",
      "\n",
      "    accuracy                           0.77     12000\n",
      "   macro avg       0.77      0.77      0.77     12000\n",
      "weighted avg       0.77      0.77      0.77     12000\n",
      "\n",
      "0.7717256746719485, 0.77925, 0.7754695857693744\n",
      "0.7048673705897502, 0.68425, 0.6944056831155653\n",
      "0.8398724865129966, 0.85625, 0.8479821738053974\n",
      "0.7721551772582318, 0.77325, 0.7726191475634456\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "SVM_pred_vanilla = SVM_vanilla.predict(X_test_tfidf_vanilla)\n",
    "print(classification_report(Y_test_vanilla, SVM_pred_vanilla))\n",
    "\n",
    "# class-wise precision, recall and f1-score\n",
    "SVM_report_vanilla = classification_report(Y_test_vanilla, SVM_pred_vanilla, output_dict=True)\n",
    "print(f\"{SVM_report_vanilla['1']['precision']}, {SVM_report_vanilla['1']['recall']}, {SVM_report_vanilla['1']['f1-score']}\")\n",
    "print(f\"{SVM_report_vanilla['2']['precision']}, {SVM_report_vanilla['2']['recall']}, {SVM_report_vanilla['2']['f1-score']}\")\n",
    "print(f\"{SVM_report_vanilla['3']['precision']}, {SVM_report_vanilla['3']['recall']}, {SVM_report_vanilla['3']['f1-score']}\")\n",
    "print(f\"{SVM_report_vanilla['macro avg']['precision']}, {SVM_report_vanilla['macro avg']['recall']}, {SVM_report_vanilla['macro avg']['f1-score']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stop Word Removal and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8331458333333334\n",
      "0.7536666666666667\n"
     ]
    }
   ],
   "source": [
    "SVM_processed = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
    "SVM_processed.fit(X_train_tfidf_processed, Y_train_processed)\n",
    "\n",
    "# training accuracy\n",
    "print(SVM_processed.score(X_train_tfidf_processed, Y_train_processed))\n",
    "\n",
    "# test accuracy\n",
    "print(SVM_processed.score(X_test_tfidf_processed, Y_test_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.76      0.76      4000\n",
      "           2       0.68      0.65      0.67      4000\n",
      "           3       0.82      0.84      0.83      4000\n",
      "\n",
      "    accuracy                           0.75     12000\n",
      "   macro avg       0.75      0.75      0.75     12000\n",
      "weighted avg       0.75      0.75      0.75     12000\n",
      "\n",
      "0.750122970978849, 0.7625, 0.7562608480039673\n",
      "0.6826597131681877, 0.6545, 0.6682833439693681\n",
      "0.8236155159795072, 0.844, 0.8336831707618224\n",
      "0.7521327333755147, 0.7536666666666666, 0.7527424542450527\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "SVM_pred_processed = SVM_processed.predict(X_test_tfidf_processed)\n",
    "print(classification_report(Y_test_processed, SVM_pred_processed))\n",
    "\n",
    "# class-wise precision, recall and f1-score\n",
    "SVM_report_processed = classification_report(Y_test_processed, SVM_pred_processed, output_dict=True)\n",
    "print(f\"{SVM_report_processed['1']['precision']}, {SVM_report_processed['1']['recall']}, {SVM_report_processed['1']['f1-score']}\")\n",
    "print(f\"{SVM_report_processed['2']['precision']}, {SVM_report_processed['2']['recall']}, {SVM_report_processed['2']['f1-score']}\")\n",
    "print(f\"{SVM_report_processed['3']['precision']}, {SVM_report_processed['3']['recall']}, {SVM_report_processed['3']['f1-score']}\")\n",
    "print(f\"{SVM_report_processed['macro avg']['precision']}, {SVM_report_processed['macro avg']['recall']}, {SVM_report_processed['macro avg']['f1-score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Stop Word Removal and Without Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8291041666666666\n",
      "0.78325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_vanilla = LogisticRegression(multi_class='multinomial', max_iter=10000)\n",
    "lr_vanilla.fit(X_train_tfidf_vanilla, Y_train_vanilla)\n",
    "\n",
    "# training accuracy\n",
    "print(lr_vanilla.score(X_train_tfidf_vanilla, Y_train_vanilla))\n",
    "\n",
    "# test accuracy\n",
    "print(lr_vanilla.score(X_test_tfidf_vanilla, Y_test_vanilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.78      0.78      4000\n",
      "           2       0.71      0.71      0.71      4000\n",
      "           3       0.86      0.85      0.86      4000\n",
      "\n",
      "    accuracy                           0.78     12000\n",
      "   macro avg       0.78      0.78      0.78     12000\n",
      "weighted avg       0.78      0.78      0.78     12000\n",
      "\n",
      "0.7806645016237822, 0.78125, 0.7809571410720979\n",
      "0.7085813492063492, 0.71425, 0.7114043824701196\n",
      "0.8617906683480454, 0.85425, 0.8580037664783428\n",
      "0.7836788397260589, 0.7832500000000001, 0.78345509667352\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "lr_pred_vanilla = lr_vanilla.predict(X_test_tfidf_vanilla)\n",
    "print(classification_report(Y_test_vanilla, lr_pred_vanilla))\n",
    "\n",
    "# class-wise precision, recall and f1-score\n",
    "lr_report_vanilla = classification_report(Y_test_vanilla, lr_pred_vanilla, output_dict=True)\n",
    "print(f\"{lr_report_vanilla['1']['precision']}, {lr_report_vanilla['1']['recall']}, {lr_report_vanilla['1']['f1-score']}\")\n",
    "print(f\"{lr_report_vanilla['2']['precision']}, {lr_report_vanilla['2']['recall']}, {lr_report_vanilla['2']['f1-score']}\")\n",
    "print(f\"{lr_report_vanilla['3']['precision']}, {lr_report_vanilla['3']['recall']}, {lr_report_vanilla['3']['f1-score']}\")\n",
    "print(f\"{lr_report_vanilla['macro avg']['precision']}, {lr_report_vanilla['macro avg']['recall']}, {lr_report_vanilla['macro avg']['f1-score']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stop Word Removal and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8192916666666666\n",
      "0.7645\n"
     ]
    }
   ],
   "source": [
    "lr_processed = LogisticRegression(multi_class='multinomial', max_iter=10000)\n",
    "lr_processed.fit(X_train_tfidf_processed, Y_train_processed)\n",
    "\n",
    "# training accuracy\n",
    "print(lr_processed.score(X_train_tfidf_processed, Y_train_processed))\n",
    "\n",
    "# test accuracy\n",
    "print(lr_processed.score(X_test_tfidf_processed, Y_test_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.77      0.77      4000\n",
      "           2       0.69      0.69      0.69      4000\n",
      "           3       0.84      0.84      0.84      4000\n",
      "\n",
      "    accuracy                           0.76     12000\n",
      "   macro avg       0.76      0.76      0.76     12000\n",
      "weighted avg       0.76      0.76      0.76     12000\n",
      "\n",
      "0.766541822721598, 0.7675, 0.7670206121174266\n",
      "0.6879076768690416, 0.6855, 0.6867017280240422\n",
      "0.8386131204789224, 0.8405, 0.8395555000624297\n",
      "0.764354206689854, 0.7645, 0.7644259467346327\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "lr_pred_processed = lr_processed.predict(X_test_tfidf_processed)\n",
    "print(classification_report(Y_test_processed, lr_pred_processed))\n",
    "\n",
    "# class-wise precision, recall and f1-score\n",
    "lr_pred_processed = lr_processed.predict(X_test_tfidf_processed)\n",
    "\n",
    "lr_report_processed = classification_report(Y_test_processed, lr_pred_processed, output_dict=True)\n",
    "print(f\"{lr_report_processed['1']['precision']}, {lr_report_processed['1']['recall']}, {lr_report_processed['1']['f1-score']}\")\n",
    "print(f\"{lr_report_processed['2']['precision']}, {lr_report_processed['2']['recall']}, {lr_report_processed['2']['f1-score']}\")\n",
    "print(f\"{lr_report_processed['3']['precision']}, {lr_report_processed['3']['recall']}, {lr_report_processed['3']['f1-score']}\")\n",
    "print(f\"{lr_report_processed['macro avg']['precision']}, {lr_report_processed['macro avg']['recall']}, {lr_report_processed['macro avg']['f1-score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Stop Word Removal and Without Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7667083333333333\n",
      "0.7415833333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB_vanilla = MultinomialNB()\n",
    "MNB_vanilla.fit(X_train_tfidf_vanilla, Y_train_vanilla)\n",
    "\n",
    "# training accuracy\n",
    "print(MNB_vanilla.score(X_train_tfidf_vanilla, Y_train_vanilla))\n",
    "\n",
    "# test accuracy\n",
    "print(MNB_vanilla.score(X_test_tfidf_vanilla, Y_test_vanilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.74      0.75      4000\n",
      "           2       0.66      0.68      0.67      4000\n",
      "           3       0.80      0.81      0.80      4000\n",
      "\n",
      "    accuracy                           0.74     12000\n",
      "   macro avg       0.74      0.74      0.74     12000\n",
      "weighted avg       0.74      0.74      0.74     12000\n",
      "\n",
      "0.7639462809917356, 0.7395, 0.7515243902439025\n",
      "0.6599465630313335, 0.67925, 0.6694591597880991\n",
      "0.8037895786586886, 0.806, 0.804893271751342\n",
      "0.742560807560586, 0.7415833333333334, 0.7419589405944479\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "MNB_pred_vanilla = MNB_vanilla.predict(X_test_tfidf_vanilla)\n",
    "print(classification_report(Y_test_vanilla, MNB_pred_vanilla))\n",
    "\n",
    "# class-wise precision, recall and f1-score\n",
    "MNB_report_vanilla = classification_report(Y_test_vanilla, MNB_pred_vanilla, output_dict=True)\n",
    "print(f\"{MNB_report_vanilla['1']['precision']}, {MNB_report_vanilla['1']['recall']}, {MNB_report_vanilla['1']['f1-score']}\")\n",
    "print(f\"{MNB_report_vanilla['2']['precision']}, {MNB_report_vanilla['2']['recall']}, {MNB_report_vanilla['2']['f1-score']}\")\n",
    "print(f\"{MNB_report_vanilla['3']['precision']}, {MNB_report_vanilla['3']['recall']}, {MNB_report_vanilla['3']['f1-score']}\")\n",
    "print(f\"{MNB_report_vanilla['macro avg']['precision']}, {MNB_report_vanilla['macro avg']['recall']}, {MNB_report_vanilla['macro avg']['f1-score']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stop Word Removal and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568958333333333\n",
      "0.7286666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB_processed = MultinomialNB()\n",
    "MNB_processed.fit(X_train_tfidf_processed, Y_train_processed)\n",
    "\n",
    "# train accuracy\n",
    "print(MNB_processed.score(X_train_tfidf_processed, Y_train_processed))\n",
    "\n",
    "# test accuracy\n",
    "print(MNB_processed.score(X_test_tfidf_processed, Y_test_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.73      0.74      4000\n",
      "           2       0.65      0.66      0.65      4000\n",
      "           3       0.78      0.80      0.79      4000\n",
      "\n",
      "    accuracy                           0.73     12000\n",
      "   macro avg       0.73      0.73      0.73     12000\n",
      "weighted avg       0.73      0.73      0.73     12000\n",
      "\n",
      "0.756078634247284, 0.73075, 0.7431985761505213\n",
      "0.6491228070175439, 0.65675, 0.6529141294892506\n",
      "0.781502324443357, 0.7985, 0.7899097316681093\n",
      "0.7289012552360616, 0.7286666666666667, 0.7286741457692938\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "MNB_pred_processed = MNB_processed.predict(X_test_tfidf_processed)\n",
    "print(classification_report(Y_test_processed, MNB_pred_processed))\n",
    "\n",
    "# class-wise precision, recall and f1-score\n",
    "MNB_report_processed = classification_report(Y_test_processed, MNB_pred_processed, output_dict=True)\n",
    "print(f\"{MNB_report_processed['1']['precision']}, {MNB_report_processed['1']['recall']}, {MNB_report_processed['1']['f1-score']}\")\n",
    "print(f\"{MNB_report_processed['2']['precision']}, {MNB_report_processed['2']['recall']}, {MNB_report_processed['2']['f1-score']}\")\n",
    "print(f\"{MNB_report_processed['3']['precision']}, {MNB_report_processed['3']['recall']}, {MNB_report_processed['3']['f1-score']}\")\n",
    "print(f\"{MNB_report_processed['macro avg']['precision']}, {MNB_report_processed['macro avg']['recall']}, {MNB_report_processed['macro avg']['f1-score']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5214815e39ece301ab6c8eb07f95d577068cbcda8864741b453ec41be186106"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
