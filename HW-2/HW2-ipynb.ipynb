{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, get_vocab = False, separate_sentences = False, replace_unkown = False, vocabulary = None, tags_present = True):\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "    lines.append(\"\\n\")\n",
    "    data = []                                       # stores each sentence as a list that contains each word as [position_index, word, tag]\n",
    "    data_df = []                                    # final dataframe\n",
    "    temp = [['0', '<START>', '<START_TAG>']]        # default entry at the start of every sentence\n",
    "    if replace_unkown:                              # for reading data to be tested\n",
    "        vocab_words = set(vocabulary.unique_words.tolist())\n",
    "\n",
    "    if get_vocab:\n",
    "        vocab = []                                  # initializing the vocabulary\n",
    "\n",
    "    for line in lines:\n",
    "        if line == '\\n':                            # if end of sentence\n",
    "            data.append(temp)\n",
    "            temp = [['0', '<START>', '<START_TAG>']]        # replace end of sentence with START (indicates START of next sentence)\n",
    "        else:\n",
    "            line = line.split('\\t')\n",
    "            if get_vocab:\n",
    "                vocab.append(line[1])\n",
    "            line[-1]=line[-1].strip('\\n')\n",
    "            if (replace_unkown) and (line[1] not in vocab_words):       # replace words not in vocabulary with <UNK>\n",
    "                line[1] = \"<UNK>\"\n",
    "            temp.append(line)\n",
    "        \n",
    "    if get_vocab:\n",
    "        vocab = pd.DataFrame(vocab, columns=[\"words\"])          # store vocabulary as a Dataframe\n",
    "\n",
    "    for data_sample in data:\n",
    "        data_df.append(pd.DataFrame(data_sample, columns=['position_index', 'word', 'tag']))            # write the data sentence by sentence -> list of dataframes\n",
    "        if not tags_present:\n",
    "            data_df[-1] = data_df[-1].drop(columns = 'tag')\n",
    "\n",
    "    data_df_combined = pd.concat(data_df)                       # collapse each sentence to occur one after the other -> 1 dataframe\n",
    "\n",
    "    if get_vocab and not separate_sentences:\n",
    "        return(data_df_combined, vocab)\n",
    "    if get_vocab and separate_sentences:\n",
    "        return(data_df, vocab)\n",
    "    if not get_vocab and not separate_sentences:\n",
    "        return(data_df_combined)\n",
    "    else:\n",
    "        return(data_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roughly takes 15s\n",
    "train_data, train_vocab = read_data('data/train', get_vocab=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, vocab_data, count_threshold=2):\n",
    "    vocab_no_dupl = vocab_data['words'].value_counts()\n",
    "    vocab_no_dupl = pd.DataFrame(list(zip(vocab_no_dupl.index.tolist(), vocab_no_dupl.tolist())), columns=['unique_words', 'count'])\n",
    "    total_words_before = len(vocab_no_dupl)\n",
    "    # print(total_words_before)\n",
    "\n",
    "    words_to_remove = vocab_no_dupl[vocab_no_dupl['count'] < count_threshold]\n",
    "    total_removed_words = len(words_to_remove)\n",
    "    vocabulary = vocab_no_dupl.drop(vocab_no_dupl[vocab_no_dupl['count'] < count_threshold].index)\n",
    "\n",
    "    vocabulary.loc[-1] = ['<UNK>', total_removed_words]  # adding a row\n",
    "    vocabulary.index = vocabulary.index + 1  # shifting index\n",
    "    vocabulary.sort_index(inplace=True) \n",
    "\n",
    "\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    s = data['word'].value_counts()\n",
    "    data['word'] = np.where(data['word'].isin(s.index[s < count_threshold]), '<UNK>', data['word'])\n",
    "\n",
    "    print(f\"Selected threshold: {count_threshold}\\nTotal Size of Vocabulary: {vocabulary_size}\\nNumber of occurences of <UNK>: {total_removed_words}\")\n",
    "    \n",
    "    vocabulary['position_index'] = vocabulary.index\n",
    "\n",
    "    return (data, vocabulary, vocabulary_size, total_words_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected threshold: 2\n",
      "Total Size of Vocabulary: 23183\n",
      "Number of occurences of <UNK>: 20011\n"
     ]
    }
   ],
   "source": [
    "train_data, vocabulary, vocabulary_size, total_words_before = create_vocab(train_data, train_vocab, count_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.to_csv(\"data/vocab.txt\", index=None, header=None, sep='\\t', columns=['unique_words', 'position_index', 'count'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Learning - Calculating Transition and Emission Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, vocabulary = None, data = None) -> None:\n",
    "        self.vocabulary = vocabulary\n",
    "        self.data = data\n",
    "        pass\n",
    "\n",
    "    def generate_helpers(self):                                         # generate the list of tags, list of words, count for each tag\n",
    "        self.data_dict = self.data.to_dict('records')\n",
    "        self.unique_words = self.vocabulary['unique_words'].to_numpy()\n",
    "        self.data['tag'].value_counts()\n",
    "        self.tag_count = dict(zip(self.data['tag'].value_counts().index.tolist(), self.data['tag'].value_counts().tolist()))\n",
    "\n",
    "        self.tag_list = self.data['tag'].unique().tolist()\n",
    "        self.word_list = self.vocabulary['unique_words'].tolist()\n",
    "        self.word_list.append(\"<START>\")                                # add <START> to the vocabulary\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        self.generate_helpers()\n",
    "\n",
    "        self.transition_count = {key:{key2:0 for key2 in self.tag_list} for key in self.tag_list}           # initialize dictionary to keep track of transition counts\n",
    "        self.emission_count = {key:{key2:0 for key2 in self.word_list} for key in self.tag_list}            # initialize dictionary to keep track of emission counts\n",
    "\n",
    "        count = 0\n",
    "        temp_tag = '<START_TAG>'\n",
    "\n",
    "        temp_row = {'position_index': '0', 'word': '<START>', 'tag': '<START_TAG>'}\n",
    "        for row in self.data_dict:\n",
    "            self.emission_count[str(row['tag'])][str(row['word'])] += 1\n",
    "            self.transition_count[str(temp_tag)][str(row['tag'])] += 1\n",
    "\n",
    "            temp_tag = row['tag']\n",
    "            temp_row = row\n",
    "\n",
    "            count += 1\n",
    "            # if count % 100000 == 0:\n",
    "            #     print(f\"Completed {count} words.\")\n",
    "    \n",
    "        self.emission_count['<START_TAG>']['<UNK>'] = self.tag_count['<START_TAG>']\n",
    "        self.transition_count['<START_TAG>']['<START_TAG>'] -= 1\n",
    "\n",
    "        self.calc_probability()\n",
    "        self.convert_prob_dict_to_print()\n",
    "\n",
    "        print(f\"Number of non-zero Transition Parameters: {len(self.transition_prob_print)}\\nNumber of non-zero Emission Parameters: {len(self.emission_prob_print)}\")\n",
    "\n",
    "    \n",
    "    def calc_probability(self):\n",
    "        # to find count for each tag based on emission and transition counts\n",
    "        self.sum_transition_count = {key: sum(self.transition_count[str(key)].values()) for key in self.tag_list}\n",
    "        self.sum_emission_count = {key: sum(self.emission_count[str(key)].values()) for key in self.tag_list}\n",
    "\n",
    "        # to calculate emission probability accurately, there is a need to subtract the count since some words are set as unknown and therefore not considered\n",
    "        transition_temp_dict = {}\n",
    "        emission_temp_dict = {}\n",
    "        for key in self.tag_list:\n",
    "            transition_temp_dict[str(key)] = (self.tag_count[str(key)] - self.sum_transition_count[str(key)])\n",
    "            emission_temp_dict[str(key)] = (self.tag_count[str(key)] - self.sum_emission_count[str(key)])\n",
    "\n",
    "        self.tag_count_new = {key: val - emission_temp_dict[str(key)] for key,val in self.tag_count.items()}\n",
    "\n",
    "        # calculate transition and emission probability\n",
    "        self.transition_prob = {key: {key2: val/self.tag_count[str(key)] for key2, val  in self.transition_count[str(key)].items()} for key in self.tag_list}\n",
    "        self.emission_prob = {key: {key2: val/self.tag_count_new[str(key)] for key2, val  in self.emission_count[str(key)].items()} for key in self.tag_list}\n",
    "        \n",
    "        print(\"Done Training\")\n",
    "\n",
    "        # return (self.transition_prob, self.emission_prob)\n",
    "\n",
    "    def convert_prob_dict_to_print(self):\n",
    "        self.transition_prob_print = {}\n",
    "        self.emission_prob_print = {}\n",
    "        for tag1 in self.transition_prob.keys():\n",
    "            for tag2 in self.transition_prob[tag1].keys():\n",
    "                transmission_value = self.transition_prob[tag1][tag2]\n",
    "                if (transmission_value != 0):\n",
    "                    self.transition_prob_print[f\"({tag1}, {tag2})\"] = transmission_value\n",
    "            for word in self.emission_prob[tag1].keys():\n",
    "                emission_value = self.emission_prob[tag1][word]\n",
    "                if (emission_value != 0):\n",
    "                    self.emission_prob_print[f\"({tag1}, {word})\"] = emission_value\n",
    "        \n",
    "    def convert_print_to_prob_dict(self):\n",
    "        self.transition_prob = {key:{key2:0 for key2 in self.tag_list} for key in self.tag_list}           # initialize dictionary to keep track of transition prob\n",
    "        self.emission_prob = {key:{key2:0 for key2 in self.word_list} for key in self.tag_list}            # initialize dictionary to keep track of emission prob\n",
    "        \n",
    "        for word, prob in self.transition_prob_print.items():\n",
    "            word = word[1:-1].split(\" \")\n",
    "            self.transition_prob[word[0][:-1]][word[1]] = prob\n",
    "\n",
    "        for word, prob in self.emission_prob_print.items():\n",
    "            word = word[1:-1].split(\" \")\n",
    "            self.emission_prob[word[0][0:-1]][word[1]] = prob\n",
    "\n",
    "    def check_prob_sum(self):                                                           # to check if sum of each probability is 1\n",
    "        sum_transition_prob = {key: sum(self.transition_prob[str(key)].values()) for key in self.tag_list}\n",
    "        sum_emission_prob = {key: sum(self.emission_prob[str(key)].values()) for key in self.tag_list}\n",
    "\n",
    "        return (sum_transition_prob, sum_emission_prob)\n",
    "\n",
    "    def load_hmm(self, filepath):\n",
    "        with open(filepath) as json_file:\n",
    "            hmm = json.load(json_file)\n",
    "        \n",
    "        self.transition_prob_print = hmm['transition']\n",
    "        self.emission_prob_print = hmm['emission']\n",
    "        self.convert_print_to_prob_dict()\n",
    "        self.tag_list = list(self.transition_prob.keys())\n",
    "\n",
    "        return (self.transition_prob, self.emission_prob)\n",
    "\n",
    "    def get_probability(self):\n",
    "        return (self.transition_prob, self.emission_prob)\n",
    "\n",
    "    def write_hmm_into_json(self, filepath):\n",
    "        hmm = {'transition': self.transition_prob_print, 'emission': self.emission_prob_print}\n",
    "\n",
    "        with open(filepath, \"w\") as json_file:\n",
    "            json.dump(hmm, json_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(vocabulary, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Training\n",
      "\n",
      "Number of non-zero Transition Parameters: 1416\n",
      "Number of non-zero Emission Parameters: 30305\n"
     ]
    }
   ],
   "source": [
    "hmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416, 30305\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(hmm.transition_prob_print)}, {len(hmm.emission_prob_print)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.write_hmm_into_json(\"data/hmm.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_prob, emission_prob = hmm.load_hmm(\"data/hmm.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = read_data('data/dev', get_vocab=False, separate_sentences=True, replace_unkown=True, vocabulary=hmm.vocabulary)\n",
    "dev_data_orig = read_data('data/dev', get_vocab=False, separate_sentences=True, vocabulary=hmm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyDecoding:\n",
    "    def __init__(self, data, tag_list, transition_prob, emission_prob, data_orig) -> None:\n",
    "        self.data = data\n",
    "        self.data_orig = data_orig\n",
    "        self.transition_prob = transition_prob\n",
    "        self.emission_prob = emission_prob\n",
    "        self.tag_list = tag_list\n",
    "        pass\n",
    "\n",
    "    def predict_sentence(self, sent_df):\n",
    "        temp_tag = \"<START_TAG>\"\n",
    "        sent = sent_df['word'].values.tolist()[1:]\n",
    "        self.sent_predictions = []\n",
    "\n",
    "        temp_prob = 1\n",
    "        for word in sent:\n",
    "            max_temp = -1\n",
    "            for tag in self.tag_list:\n",
    "                temp_prob = self.transition_prob[temp_tag][tag] * self.emission_prob[tag][word]\n",
    "                if temp_prob > max_temp:\n",
    "                    max_temp = temp_prob\n",
    "                    corresponding_tag = tag\n",
    "            \n",
    "            temp_tag = corresponding_tag\n",
    "            \n",
    "            self.sent_predictions.append(corresponding_tag)\n",
    "\n",
    "        return(self.sent_predictions, sent)\n",
    "    \n",
    "    def predict(self):\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        self.to_write = []\n",
    "        \n",
    "        count = 0\n",
    "        for sent in self.data:\n",
    "            sent_orig = self.data_orig[count]['word'].values.tolist()[1:]\n",
    "            predictions, sent = self.predict_sentence(sent)\n",
    "            pos = 1\n",
    "            for pred, word in zip(predictions, sent_orig):\n",
    "                self.to_write.append(f\"{pos}\\t{word}\\t{pred}\\n\")\n",
    "                pos += 1\n",
    "            self.predictions.extend(predictions)\n",
    "            self.to_write.append(\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "        self.to_write = \"\".join(self.to_write[:-1])\n",
    "        return self.predictions\n",
    "\n",
    "\n",
    "    def calc_score(self, targets):\n",
    "        count_of_matches = 0\n",
    "        for pred, target in zip(self.predictions, targets):\n",
    "            if pred == target:\n",
    "                count_of_matches += 1\n",
    "        self.accuracy = count_of_matches / len(self.predictions)\n",
    "        return self.accuracy\n",
    "    \n",
    "    def get_targets(self):\n",
    "        self.targets = []\n",
    "        for sent_df in self.data:\n",
    "            self.targets.extend(sent_df['tag'].values.tolist()[1:])\n",
    "        return self.targets\n",
    "\n",
    "    def write_prediction_into_file(self, filepath):\n",
    "        with open(filepath, \"w\") as output_file:\n",
    "            output_file.write(self.to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_prob, emission_prob = hmm.get_probability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_dev = GreedyDecoding(dev_data, hmm.tag_list, transition_prob, emission_prob, dev_data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roughly takes 3.5s\n",
    "preds = greedy_dev.predict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy Decoding Accuracy Score: 0.934870378240544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Decoding Accuracy on dev_data: 93.4870378240544\n"
     ]
    }
   ],
   "source": [
    "acc = greedy_dev.calc_score(greedy_dev.get_targets())\n",
    "print(f\"Greedy Decoding Accuracy on dev_data: {acc*100}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Viterbi Decoding Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViterbiDecoding:\n",
    "    def __init__(self, data, tag_list, transition_prob, emission_prob, data_orig) -> None:\n",
    "        self.data = data\n",
    "        self.data_orig = data_orig\n",
    "        self.transition_prob = transition_prob\n",
    "        self.emission_prob = emission_prob\n",
    "        self.tag_list = tag_list[1:]\n",
    "        self.map_tag_to_index()\n",
    "        self.map_index_to_tag()\n",
    "        pass\n",
    "\n",
    "    def map_tag_to_index(self):\n",
    "        self.tag_to_index = {}\n",
    "        i = 0\n",
    "        for tag in self.tag_list:\n",
    "            self.tag_to_index[tag] = i\n",
    "            i += 1\n",
    "\n",
    "    def map_index_to_tag(self):\n",
    "        self.index_to_tag = {v: k for k, v in self.tag_to_index.items()}\n",
    "\n",
    "    def predict(self):\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        self.to_write = []\n",
    "\n",
    "        count = 0\n",
    "        for sent in self.data:\n",
    "            sent_orig = self.data_orig[count]['word'].values.tolist()[1:]\n",
    "            \n",
    "            predictions, sent = self.predict_sentence(sent)\n",
    "            pos = 1\n",
    "            for pred, word in zip(predictions, sent_orig):\n",
    "                self.to_write.append(f\"{pos}\\t{word}\\t{pred}\\n\")\n",
    "                pos += 1\n",
    "            self.predictions.extend(predictions)\n",
    "            \n",
    "            self.to_write.append(\"\\n\")\n",
    "            \n",
    "            count += 1\n",
    "            # if count % 500 == 0:\n",
    "            #     print(f\"Completed {count} sentences.\")\n",
    "        \n",
    "        self.to_write = \"\".join(self.to_write[:-1])\n",
    "        \n",
    "        return self.predictions\n",
    "\n",
    "    def calc_score(self, targets):\n",
    "        count_of_matches = 0\n",
    "        for pred, target in zip(self.predictions, targets):\n",
    "            if pred == target:\n",
    "                count_of_matches += 1\n",
    "\n",
    "        self.accuracy = count_of_matches / len(self.predictions)\n",
    "        return self.accuracy\n",
    "    \n",
    "    def get_targets(self):\n",
    "        self.targets = []\n",
    "        for sent_df in self.data:\n",
    "            self.targets.extend(sent_df['tag'].values.tolist()[1:])\n",
    "        return self.targets\n",
    "\n",
    "    \n",
    "    def predict_sentence(self, sent_df):\n",
    "        self.sent = sent_df['word'].values.tolist()[1:]\n",
    "\n",
    "        sentence_length = len(self.sent)\n",
    "        no_of_tags = len(self.tag_list)\n",
    "\n",
    "        self.OPT = np.zeros((no_of_tags, sentence_length))\n",
    "        self.backtrack_matrix = np.zeros((no_of_tags, sentence_length))\n",
    "\n",
    "        index = 0\n",
    "        for tag in self.tag_list:\n",
    "            self.OPT[self.tag_to_index[tag], index] = self.transition_prob['<START_TAG>'][tag] * self.emission_prob[tag][self.sent[index]]\n",
    "\n",
    "        for j in range(1, sentence_length):\n",
    "            for current_tag in self.tag_list:\n",
    "                temp_prob = []\n",
    "                for previous_tag in self.tag_list:\n",
    "                    temp_prob.append(self.OPT[self.tag_to_index[previous_tag], j-1] * self.transition_prob[previous_tag][current_tag] * self.emission_prob[current_tag][self.sent[j]])\n",
    "                \n",
    "                max_tag_index = np.argmax(temp_prob)\n",
    "\n",
    "                self.OPT[self.tag_to_index[current_tag], j] = temp_prob[max_tag_index]\n",
    "                self.backtrack_matrix[self.tag_to_index[current_tag], j] = max_tag_index\n",
    "\n",
    "        pred_tags = self.backtrack(self.OPT, self.backtrack_matrix)\n",
    "\n",
    "        return (pred_tags, self.sent)\n",
    "\n",
    "    def backtrack(self, OPT, bactrack_matrix):\n",
    "        pred_tag = []\n",
    "        sentence_length = len(self.sent)\n",
    "        no_of_tags = len(self.tag_list)\n",
    "        \n",
    "        j = sentence_length - 1\n",
    "        index = np.argmax(OPT[:,j])\n",
    "        pointer = bactrack_matrix[index, j]\n",
    "        pred_tag.append(self.index_to_tag[index])\n",
    "\n",
    "        for j in range(sentence_length-2, -1, -1):\n",
    "            pred_tag.append(self.index_to_tag[pointer])\n",
    "            pointer = bactrack_matrix[int(pointer), j]\n",
    "\n",
    "        pred_tag.reverse()\n",
    "\n",
    "        return pred_tag\n",
    "    \n",
    "    def write_prediction_into_file(self, filepath):\n",
    "        with open(filepath, \"w\") as output_file:\n",
    "            output_file.write(self.to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_dev = ViterbiDecoding(dev_data, hmm.tag_list, transition_prob, emission_prob, dev_data_orig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of sentences: ~5500 \n",
    "\n",
    "- Time taken (roughly) for all words: 3m 45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of sentences: ~5500 \n",
    "# Time taken (roughly) for all words: 3m 45s\n",
    "preds = viterbi_dev.predict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi Decoding Accuracy Score: 0.9476883613623945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Decoding Accuracy on dev_data: 94.76883613623946\n"
     ]
    }
   ],
   "source": [
    "acc = viterbi_dev.calc_score(viterbi_dev.get_targets())\n",
    "print(f\"Viterbi Decoding Accuracy on dev_data: {acc*100}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_data('data/test', get_vocab=False, separate_sentences=True, replace_unkown=True, vocabulary=hmm.vocabulary, tags_present=False)\n",
    "test_data_orig = read_data('data/test', get_vocab=False, separate_sentences=True, vocabulary=hmm.vocabulary, tags_present=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching transition and emission probabilities from the saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_prob, emission_prob = hmm.load_hmm(\"data/hmm.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decoding for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_test = GreedyDecoding(test_data, hmm.tag_list, transition_prob, emission_prob, test_data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken (roughly) for all words: 3s\n",
    "preds = greedy_test.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_test.write_prediction_into_file(\"data/greedy.out\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Decoding for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_test = ViterbiDecoding(test_data, hmm.tag_list, transition_prob, emission_prob, test_data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken (roughly) for all words: 3m 45s\n",
    "preds = viterbi_test.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_test.write_prediction_into_file(\"data/viterbi.out\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5214815e39ece301ab6c8eb07f95d577068cbcda8864741b453ec41be186106"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
